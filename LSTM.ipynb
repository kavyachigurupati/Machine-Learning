{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvHTBEvlqR41"
   },
   "source": [
    "**Name: Kavya Chigurupati**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZQURFk6WsgT",
    "outputId": "114096c0-c80c-43ac-a7e9-4b54b48776c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AmpvFq9NahJ3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ov7317d8axoS"
   },
   "outputs": [],
   "source": [
    "# CORA='/content/gdrive/MyDrive/CORA/'\n",
    "CORA ='/content/gdrive/MyDrive/ITCS-6156-ML/CORA/'\n",
    "\n",
    "with open(CORA + 'vocab-lower.pickle', 'rb') as f:\n",
    "    #vocab dic is list of all words in the dictionery\n",
    "    vocab_dic = list(map(lambda x: x.decode('utf-8'), pickle.load(f, encoding='utf-8')))\n",
    "    # convert all the data into lower case\n",
    "\n",
    "# assign token id to each word and add it to pickle-kc\n",
    "with open(CORA + 'vocab_labels.pickle', 'rb') as f:\n",
    "    #The list of possible labels \n",
    "    label_dic = list(map(lambda x: x.decode('utf-8'), pickle.load(f, encoding='bytes')))\n",
    "\n",
    "# remove tokens that occur only once - but here since it is citation and may have names we didnot do it\n",
    "    \n",
    "with open(CORA + 'trained_embedding.pickle', 'rb') as f:\n",
    "    #The trained embedding on Wikipedia using GLOVE algorithm \n",
    "    #Each column is the embedding of a word (with respect to the vocab dictionary) \n",
    "    embedding = pickle.load(f, encoding='bytes')\n",
    "    vocabulary_size = 20608\n",
    "    embedding_size = 100\n",
    "\n",
    "\n",
    "with open(CORA + 'xdata-lower.pickle', 'rb') as f:\n",
    "    #Each row is a citation. Each cell is the token id (with respect to the vocab dictionary) within the citation.\n",
    "    xdata = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_train.pickle', 'rb') as f:\n",
    "    ydata = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'xval-lower.pickle', 'rb') as f:\n",
    "    xval = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_dev.pickle', 'rb') as f:\n",
    "    yval = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'xtest-lower.pickle', 'rb') as f:\n",
    "    xtest = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(CORA + 'Y_test.pickle', 'rb') as f:\n",
    "    ytest = pickle.load(f, encoding='bytes')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fi85Z0r3tNw",
    "outputId": "ce9809c2-6b1e-4048-e45b-b1a2cb55a01f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18049"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ec0uMB94FVS",
    "outputId": "274aa061-9903-458e-96d8-8fc6f901d92c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ',', '-', 'and', 'of', 'in', '(', ')', 'a']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dic[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbTdXxqWSzgp",
    "outputId": "3d44c2d7-5e94-470b-bf24-f795d3dc80e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXT9_0mqTNWP",
    "outputId": "6709f215-b0de-44fd-8c47-f5e3545f9d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "[    9     1  5259     2    22     1  8920     2     4    41     1     3\n",
      "    26     1   161  3309     1 16305  6634   277  1933   970 11234  2751\n",
      "     1     6    25     1    40     1   709     2    22     1    25     1\n",
      " 11654     2     4    38     1  7646     2   103     2    29     1   357\n",
      "     1  5710     3  9418  1846    69     2    93     1     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(len(xdata[0]))\n",
    "print(xdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1UdUc2t5Wp0",
    "outputId": "5c98bdcc-e154-4962-cfd9-b375aefbeed9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4, 11, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTXl5Ob2bQtC",
    "outputId": "e6ef5ffe-866a-40cc-8ba3-7e75df537bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 18049\n",
      "no formal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"vocab size:\", len(vocab_dic))\n",
    "print(vocab_dic[100], vocab_dic[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9Z09S8ClbVvw"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "  norm_v1 = np.linalg.norm(v1)\n",
    "  norm_v2 = np.linalg.norm(v2)\n",
    "  return np.dot(v1,v2)/(norm_v1*norm_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "KGKg5ll-cU4O"
   },
   "outputs": [],
   "source": [
    "v1_id = vocab_dic.index('positive')\n",
    "v2_id = vocab_dic.index('constructive')\n",
    "v3_id = vocab_dic.index('conference')\n",
    "\n",
    "v1_vec = embedding[v1_id]\n",
    "v2_vec = embedding[v2_id]\n",
    "v3_vec = embedding[v3_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDCz_KavtSjv",
    "outputId": "0ce4455b-2c68-4adb-b21f-5183052bc465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4659\n",
      "[ 0.54177    1.0436     0.55266   -0.62825    0.30594    0.28087\n",
      " -0.35377    0.35549    0.95647    0.50894   -0.26715    0.55767\n",
      " -1.2315     0.40356   -0.096982  -0.086239  -0.14343    0.17824\n",
      " -0.041343  -1.0227    -0.17425   -0.61138    0.8514     0.27748\n",
      "  0.0053477 -0.6192    -0.84856   -0.51399   -0.49071   -0.43858\n",
      "  0.88657   -0.50032   -0.54597   -0.072396  -0.55092    0.17349\n",
      "  0.019984  -0.070796  -0.081909  -0.49314   -0.51248    0.50913\n",
      "  0.3783     1.2291     0.20437    0.012887   0.32293   -0.21534\n",
      "  0.5537     0.80678   -0.52994    0.46817   -0.81708   -0.018085\n",
      " -0.60689   -0.83048   -0.3863     0.22914    0.52167   -0.1351\n",
      "  0.23068    0.20365    0.53207   -0.28598    0.32816   -0.28617\n",
      " -0.465      0.3136    -0.82072   -0.3596    -1.0122    -0.33755\n",
      " -0.10101   -0.13263   -1.0847    -0.6569    -0.45667   -0.52978\n",
      " -0.32365    1.0648    -0.53276    0.43342    0.61473   -0.79105\n",
      "  0.24635   -0.677      0.68037   -0.79789   -0.27437    0.79626\n",
      "  0.59855    0.075102   0.42128   -0.69344   -0.24996   -0.261\n",
      "  0.12454    0.31098    0.27499   -0.31943  ]\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(v1_id)\n",
    "print(v1_vec)\n",
    "print(len(v1_vec))\n",
    "print(len(v2_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmLEhiIxc7FO",
    "outputId": "3f859135-c3a8-4577-8a1a-88ee50fc6097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829688468111025"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1_vec,v2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx5u_An5eMTp",
    "outputId": "6c2a52cc-cdf0-4fa6-958b-ee9174add66e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.226298426910638"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1_vec, v3_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnxWHKR0ePbV",
    "outputId": "ef291f5f-9735-461e-cbc4-f1b1d9ac3ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   16     1  4426     2    33     1  2014     2     4    38     1  8853\n",
      "     3 14152     1   189     5  1901    37    59   839   128     4   319\n",
      "   155     1   102   282    62     2    46     7    46     8     2   297\n",
      "     1     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#construct the sentence\n",
    "print(xdata[1])\n",
    "#each cell is the token id and 0 means padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "KPfk5QVjebKo"
   },
   "outputs": [],
   "source": [
    "def get_sentence(x):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  return ' '.join([vocab_dic[id] for id in x_nopad])\n",
    "\n",
    "def get_label(y):\n",
    "  y_nopad = list(filter(lambda i: True if i > 0 else False, y))\n",
    "  return ' '.join([label_dic[id] for id in y_nopad])\n",
    "\n",
    "def print_sentence_label_pair(x, y):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  y_nopad = list(filter(lambda i: True if i > 0 else False, y))\n",
    "  for i in range(len(x_nopad)):\n",
    "    print(label_dic[y_nopad[i]],'\\t\\t' ,vocab_dic[x_nopad[i]],)\n",
    "\n",
    "def print_sentence_pred_label(x, y, yt):\n",
    "  x_nopad = list(filter(lambda i: True if i > 0 else False, x))\n",
    "  for i in range(len(x_nopad)):\n",
    "    print(label_dic[yt[i]], label_dic[y[i]],'\\t\\t'  ,vocab_dic[x_nopad[i]],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOap6bWNu5pb",
    "outputId": "6b5121d3-e78b-4a64-ef60-37c822264128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramadge , p . , & wonham , w . ( 1989 ) . the control of discrete event systems . proceedings of the ieee , 77 ( 1 ) , 81 - 98 .\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence(xdata[10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdmS_2A1vI83",
    "outputId": "e0c790a7-6f83-4de5-ece3-fbc32fb11bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author author author author author author author author author author date date date date title title title title title title title booktitle booktitle booktitle booktitle booktitle volume volume volume volume volume pages pages pages pages\n"
     ]
    }
   ],
   "source": [
    " print(get_label(ydata[10]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sl8DO5jJgB68",
    "outputId": "730c7f83-6872-4e7d-ec1c-860dab2a2df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author \t\t ramadge\n",
      "author \t\t ,\n",
      "author \t\t p\n",
      "author \t\t .\n",
      "author \t\t ,\n",
      "author \t\t &\n",
      "author \t\t wonham\n",
      "author \t\t ,\n",
      "author \t\t w\n",
      "author \t\t .\n",
      "date \t\t (\n",
      "date \t\t 1989\n",
      "date \t\t )\n",
      "date \t\t .\n",
      "title \t\t the\n",
      "title \t\t control\n",
      "title \t\t of\n",
      "title \t\t discrete\n",
      "title \t\t event\n",
      "title \t\t systems\n",
      "title \t\t .\n",
      "booktitle \t\t proceedings\n",
      "booktitle \t\t of\n",
      "booktitle \t\t the\n",
      "booktitle \t\t ieee\n",
      "booktitle \t\t ,\n",
      "volume \t\t 77\n",
      "volume \t\t (\n",
      "volume \t\t 1\n",
      "volume \t\t )\n",
      "volume \t\t ,\n",
      "pages \t\t 81\n",
      "pages \t\t -\n",
      "pages \t\t 98\n",
      "pages \t\t .\n"
     ]
    }
   ],
   "source": [
    "print_sentence_label_pair(xdata[10], ydata[10])\n",
    "# print(get_sentence(xdata[10]))\n",
    "# print(get_label(ydata[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "nbU7C7R1RMb4"
   },
   "outputs": [],
   "source": [
    "#calucate the hamming loss between gold data and predication\n",
    "def token_level_loss(cpred, ctrue):\n",
    "  try:\n",
    "    label_num = np.shape(cpred)[1]\n",
    "    pred = cpred.astype(np.int)\n",
    "    true = ctrue.astype(np.int)\n",
    "    n1 = len(pred)\n",
    "    n2 = len(true)\n",
    "    assert n1 == n2\n",
    "    hloss = 0.0\n",
    "    exact_acc = 0.0\n",
    "    for i in range(n1):\n",
    "      sample_loss = 0.0\n",
    "      exact = 0.0\n",
    "      xp = pred[i]\n",
    "      xt = true[i]\n",
    "      cnp = 0\n",
    "      for j in range(label_num):\n",
    "        if (xt[j] == 0):\n",
    "          continue\n",
    "        cnp+=1\n",
    "        if xp[j] != xt[j]:\n",
    "          sample_loss += 1.0\n",
    "        else:\n",
    "          exact +=1\n",
    "      sample_loss = sample_loss / cnp\n",
    "      exact = exact / cnp\n",
    "      hloss += sample_loss\n",
    "      exact_acc += exact\n",
    "    return (hloss / n1, exact_acc / n1)\n",
    "  except Warning:\n",
    "    return (0.0,0.0);\n",
    "\n",
    "def perf(ytr_pred, yval_pred, yts_pred, ydata, yval, ytest):\n",
    "    global best_val\n",
    "    global test_val\n",
    "\n",
    "    hm_ts, ex_ts = token_level_loss(yts_pred, ytest)\n",
    "    hm_tr, ex_tr = token_level_loss(ytr_pred, ydata)\n",
    "    hm_val, ex_val = token_level_loss(yval_pred, yval)\n",
    "    if ex_val > best_val:\n",
    "        best_val = ex_val\n",
    "        test_val = ex_ts\n",
    "    return (\"Train: %0.3f Val: %0.3f Test: %0.3f -- Best Val: %0.3f Test: %0.3f\" % (ex_tr, ex_val, ex_ts, best_val, test_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoUQKfa1r7TA"
   },
   "source": [
    "**Model 1** : Bidirectional Lstm \n",
    "\n",
    "Accuracy: 53.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "JSRNVhcugZZv"
   },
   "outputs": [],
   "source": [
    "output_size = len(label_dic)\n",
    "max_length_output = 118\n",
    "class CitationNetwork(tf.keras.Model):\n",
    "  def __init__(self, network_type = 'SimpleLSTM'):\n",
    "      super(CitationNetwork, self).__init__()\n",
    "\n",
    "      # self.optimizer = tf.keras.optimizers.RMSprop(1e-3)\n",
    "      self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "      self.embedding = layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_size)\n",
    "      \n",
    "      self.forward_layer = layers.LSTM(32, return_sequences=True, return_state=True)\n",
    "      self.backward_layer = layers.LSTM(32, return_sequences=True,return_state=True,go_backwards=True)\n",
    "      self.rnn = layers.Bidirectional(self.forward_layer,merge_mode=\"concat\",backward_layer = self.backward_layer)\n",
    "      \n",
    "\n",
    "      self.fc = layers.Dense(output_size)\n",
    "      \n",
    " \n",
    "  def call(self, x, predict=True):\n",
    "      emb = self.embedding(x)\n",
    "      output,_,_,_,_ = self.rnn(emb)\n",
    "      \n",
    "\n",
    "      output = self.fc(output)\n",
    "      if predict:\n",
    "        return tf.math.argmax(output, axis=-1)\n",
    "      return output\n",
    "  \n",
    "  def get_loss(self, ylogits, yt):\n",
    "    # y - one hot vector which makes it easy to compute loss \n",
    "    ylabels = tf.one_hot(ybatch, output_size)\n",
    "    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=ylogits, labels=ylabels);\n",
    "    return tf.reduce_sum(cross_ent)\n",
    "\n",
    "@tf.function\n",
    "def train_step(xbatch, ybatch):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = cite(xbatch, False)\n",
    "        # prediction = cite(xbatch, False)\n",
    "        loss = cite.get_loss(prediction, ybatch)\n",
    "        \n",
    "    gradients = tape.gradient(loss, cite.trainable_variables)\n",
    "    cite.optimizer.apply_gradients(zip(gradients, cite.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "cite = CitationNetwork();\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_91RxdU-3Xa",
    "outputId": "fc290626-2fde-4d9c-9855-4c9aafbca0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 65470.508 Train: 0.000 Val: 0.000 Test: 0.000 -- Best Val: 0.000 Test: 0.000\n",
      "1 36676.49 Train: 0.245 Val: 0.248 Test: 0.273 -- Best Val: 0.248 Test: 0.273\n",
      "2 32636.643 Train: 0.467 Val: 0.477 Test: 0.482 -- Best Val: 0.477 Test: 0.482\n",
      "3 29189.953 Train: 0.445 Val: 0.463 Test: 0.470 -- Best Val: 0.477 Test: 0.482\n",
      "4 26219.719 Train: 0.485 Val: 0.482 Test: 0.506 -- Best Val: 0.482 Test: 0.506\n",
      "5 23634.148 Train: 0.467 Val: 0.490 Test: 0.533 -- Best Val: 0.490 Test: 0.533\n",
      "6 22149.207 Train: 0.439 Val: 0.475 Test: 0.511 -- Best Val: 0.490 Test: 0.533\n",
      "7 20935.277 Train: 0.423 Val: 0.458 Test: 0.496 -- Best Val: 0.490 Test: 0.533\n",
      "8 20298.021 Train: 0.424 Val: 0.469 Test: 0.511 -- Best Val: 0.490 Test: 0.533\n",
      "9 19996.432 Train: 0.412 Val: 0.456 Test: 0.493 -- Best Val: 0.490 Test: 0.533\n",
      "10 19617.781 Train: 0.418 Val: 0.464 Test: 0.508 -- Best Val: 0.490 Test: 0.533\n",
      "11 19204.236 Train: 0.413 Val: 0.455 Test: 0.493 -- Best Val: 0.490 Test: 0.533\n",
      "12 19242.908 Train: 0.414 Val: 0.455 Test: 0.498 -- Best Val: 0.490 Test: 0.533\n",
      "13 18972.275 Train: 0.419 Val: 0.457 Test: 0.497 -- Best Val: 0.490 Test: 0.533\n",
      "14 18725.684 Train: 0.420 Val: 0.456 Test: 0.496 -- Best Val: 0.490 Test: 0.533\n",
      "15 18595.201 Train: 0.418 Val: 0.457 Test: 0.503 -- Best Val: 0.490 Test: 0.533\n",
      "16 18494.857 Train: 0.416 Val: 0.451 Test: 0.492 -- Best Val: 0.490 Test: 0.533\n",
      "17 18320.21 Train: 0.425 Val: 0.454 Test: 0.498 -- Best Val: 0.490 Test: 0.533\n",
      "18 18272.72 Train: 0.418 Val: 0.449 Test: 0.492 -- Best Val: 0.490 Test: 0.533\n",
      "19 18103.586 Train: 0.417 Val: 0.451 Test: 0.496 -- Best Val: 0.490 Test: 0.533\n",
      "20 18184.73 Train: 0.420 Val: 0.451 Test: 0.494 -- Best Val: 0.490 Test: 0.533\n",
      "21 17932.95 Train: 0.408 Val: 0.441 Test: 0.485 -- Best Val: 0.490 Test: 0.533\n",
      "22 17898.795 Train: 0.412 Val: 0.441 Test: 0.480 -- Best Val: 0.490 Test: 0.533\n",
      "23 17999.012 Train: 0.415 Val: 0.447 Test: 0.487 -- Best Val: 0.490 Test: 0.533\n",
      "24 17847.496 Train: 0.416 Val: 0.441 Test: 0.482 -- Best Val: 0.490 Test: 0.533\n",
      "25 17812.686 Train: 0.417 Val: 0.444 Test: 0.488 -- Best Val: 0.490 Test: 0.533\n",
      "26 17618.9 Train: 0.419 Val: 0.445 Test: 0.482 -- Best Val: 0.490 Test: 0.533\n",
      "27 17733.246 Train: 0.416 Val: 0.443 Test: 0.486 -- Best Val: 0.490 Test: 0.533\n",
      "28 17686.684 Train: 0.412 Val: 0.439 Test: 0.479 -- Best Val: 0.490 Test: 0.533\n",
      "29 17617.787 Train: 0.415 Val: 0.441 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "30 17667.877 Train: 0.418 Val: 0.443 Test: 0.489 -- Best Val: 0.490 Test: 0.533\n",
      "31 17521.922 Train: 0.417 Val: 0.444 Test: 0.488 -- Best Val: 0.490 Test: 0.533\n",
      "32 17468.69 Train: 0.415 Val: 0.442 Test: 0.484 -- Best Val: 0.490 Test: 0.533\n",
      "33 17435.422 Train: 0.418 Val: 0.445 Test: 0.487 -- Best Val: 0.490 Test: 0.533\n",
      "34 17435.012 Train: 0.413 Val: 0.439 Test: 0.486 -- Best Val: 0.490 Test: 0.533\n",
      "35 17384.771 Train: 0.410 Val: 0.441 Test: 0.479 -- Best Val: 0.490 Test: 0.533\n",
      "36 17399.236 Train: 0.418 Val: 0.442 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "37 17435.783 Train: 0.411 Val: 0.439 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "38 17290.828 Train: 0.414 Val: 0.442 Test: 0.482 -- Best Val: 0.490 Test: 0.533\n",
      "39 17258.143 Train: 0.419 Val: 0.445 Test: 0.489 -- Best Val: 0.490 Test: 0.533\n",
      "40 17254.791 Train: 0.417 Val: 0.442 Test: 0.482 -- Best Val: 0.490 Test: 0.533\n",
      "41 17250.152 Train: 0.419 Val: 0.442 Test: 0.485 -- Best Val: 0.490 Test: 0.533\n",
      "42 17230.973 Train: 0.417 Val: 0.439 Test: 0.482 -- Best Val: 0.490 Test: 0.533\n",
      "43 17204.441 Train: 0.417 Val: 0.440 Test: 0.477 -- Best Val: 0.490 Test: 0.533\n",
      "44 17109.941 Train: 0.418 Val: 0.439 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "45 17123.44 Train: 0.419 Val: 0.439 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "46 17094.611 Train: 0.419 Val: 0.440 Test: 0.479 -- Best Val: 0.490 Test: 0.533\n",
      "47 17161.766 Train: 0.417 Val: 0.439 Test: 0.478 -- Best Val: 0.490 Test: 0.533\n",
      "48 17074.836 Train: 0.418 Val: 0.438 Test: 0.475 -- Best Val: 0.490 Test: 0.533\n",
      "49 16981.357 Train: 0.421 Val: 0.439 Test: 0.478 -- Best Val: 0.490 Test: 0.533\n",
      "50 17061.99 Train: 0.418 Val: 0.440 Test: 0.481 -- Best Val: 0.490 Test: 0.533\n",
      "51 17010.34 Train: 0.414 Val: 0.438 Test: 0.476 -- Best Val: 0.490 Test: 0.533\n",
      "52 16985.07 Train: 0.416 Val: 0.438 Test: 0.478 -- Best Val: 0.490 Test: 0.533\n",
      "53 16965.521 Train: 0.416 Val: 0.437 Test: 0.479 -- Best Val: 0.490 Test: 0.533\n",
      "54 16973.293 Train: 0.411 Val: 0.437 Test: 0.476 -- Best Val: 0.490 Test: 0.533\n",
      "55 16944.314 Train: 0.415 Val: 0.432 Test: 0.474 -- Best Val: 0.490 Test: 0.533\n",
      "56 16914.582 Train: 0.413 Val: 0.433 Test: 0.472 -- Best Val: 0.490 Test: 0.533\n",
      "57 16938.127 Train: 0.411 Val: 0.432 Test: 0.472 -- Best Val: 0.490 Test: 0.533\n",
      "58 16874.623 Train: 0.417 Val: 0.436 Test: 0.479 -- Best Val: 0.490 Test: 0.533\n",
      "59 16902.107 Train: 0.415 Val: 0.433 Test: 0.476 -- Best Val: 0.490 Test: 0.533\n",
      "60 16880.213 Train: 0.414 Val: 0.433 Test: 0.471 -- Best Val: 0.490 Test: 0.533\n",
      "61 16856.082 Train: 0.415 Val: 0.436 Test: 0.477 -- Best Val: 0.490 Test: 0.533\n",
      "62 16846.977 Train: 0.414 Val: 0.435 Test: 0.478 -- Best Val: 0.490 Test: 0.533\n",
      "63 16864.459 Train: 0.419 Val: 0.438 Test: 0.474 -- Best Val: 0.490 Test: 0.533\n",
      "64 16841.027 Train: 0.417 Val: 0.434 Test: 0.474 -- Best Val: 0.490 Test: 0.533\n",
      "65 16820.562 Train: 0.416 Val: 0.434 Test: 0.473 -- Best Val: 0.490 Test: 0.533\n",
      "66 16832.516 Train: 0.411 Val: 0.428 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "67 16784.752 Train: 0.414 Val: 0.430 Test: 0.467 -- Best Val: 0.490 Test: 0.533\n",
      "68 16795.836 Train: 0.415 Val: 0.434 Test: 0.472 -- Best Val: 0.490 Test: 0.533\n",
      "69 16806.787 Train: 0.413 Val: 0.434 Test: 0.470 -- Best Val: 0.490 Test: 0.533\n",
      "70 16765.58 Train: 0.414 Val: 0.434 Test: 0.469 -- Best Val: 0.490 Test: 0.533\n",
      "71 16769.332 Train: 0.417 Val: 0.436 Test: 0.470 -- Best Val: 0.490 Test: 0.533\n",
      "72 16770.225 Train: 0.415 Val: 0.433 Test: 0.469 -- Best Val: 0.490 Test: 0.533\n",
      "73 16731.701 Train: 0.416 Val: 0.433 Test: 0.465 -- Best Val: 0.490 Test: 0.533\n",
      "74 16771.443 Train: 0.412 Val: 0.428 Test: 0.460 -- Best Val: 0.490 Test: 0.533\n",
      "75 16734.693 Train: 0.412 Val: 0.427 Test: 0.461 -- Best Val: 0.490 Test: 0.533\n",
      "76 16740.844 Train: 0.411 Val: 0.428 Test: 0.460 -- Best Val: 0.490 Test: 0.533\n",
      "77 16703.127 Train: 0.412 Val: 0.429 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "78 16692.89 Train: 0.412 Val: 0.432 Test: 0.471 -- Best Val: 0.490 Test: 0.533\n",
      "79 16712.422 Train: 0.410 Val: 0.429 Test: 0.466 -- Best Val: 0.490 Test: 0.533\n",
      "80 16673.354 Train: 0.413 Val: 0.430 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "81 16721.97 Train: 0.414 Val: 0.429 Test: 0.461 -- Best Val: 0.490 Test: 0.533\n",
      "82 16710.604 Train: 0.415 Val: 0.429 Test: 0.465 -- Best Val: 0.490 Test: 0.533\n",
      "83 16701.957 Train: 0.419 Val: 0.432 Test: 0.470 -- Best Val: 0.490 Test: 0.533\n",
      "84 16695.426 Train: 0.417 Val: 0.432 Test: 0.467 -- Best Val: 0.490 Test: 0.533\n",
      "85 16666.309 Train: 0.416 Val: 0.432 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "86 16665.498 Train: 0.415 Val: 0.430 Test: 0.465 -- Best Val: 0.490 Test: 0.533\n",
      "87 16689.045 Train: 0.416 Val: 0.431 Test: 0.463 -- Best Val: 0.490 Test: 0.533\n",
      "88 16673.559 Train: 0.415 Val: 0.428 Test: 0.461 -- Best Val: 0.490 Test: 0.533\n",
      "89 16619.723 Train: 0.415 Val: 0.429 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "90 16652.06 Train: 0.412 Val: 0.427 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "91 16621.578 Train: 0.415 Val: 0.431 Test: 0.467 -- Best Val: 0.490 Test: 0.533\n",
      "92 16640.188 Train: 0.413 Val: 0.431 Test: 0.468 -- Best Val: 0.490 Test: 0.533\n",
      "93 16671.988 Train: 0.410 Val: 0.427 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "94 16619.361 Train: 0.412 Val: 0.430 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "95 16617.807 Train: 0.414 Val: 0.432 Test: 0.466 -- Best Val: 0.490 Test: 0.533\n",
      "96 16610.15 Train: 0.415 Val: 0.433 Test: 0.467 -- Best Val: 0.490 Test: 0.533\n",
      "97 16603.977 Train: 0.415 Val: 0.430 Test: 0.465 -- Best Val: 0.490 Test: 0.533\n",
      "98 16595.57 Train: 0.417 Val: 0.433 Test: 0.468 -- Best Val: 0.490 Test: 0.533\n",
      "99 16601.12 Train: 0.414 Val: 0.430 Test: 0.461 -- Best Val: 0.490 Test: 0.533\n",
      "100 16613.24 Train: 0.414 Val: 0.432 Test: 0.464 -- Best Val: 0.490 Test: 0.533\n",
      "101 16566.414 Train: 0.418 Val: 0.436 Test: 0.473 -- Best Val: 0.490 Test: 0.533\n",
      "102 16567.666 Train: 0.419 Val: 0.436 Test: 0.472 -- Best Val: 0.490 Test: 0.533\n",
      "103 16579.768 Train: 0.418 Val: 0.432 Test: 0.466 -- Best Val: 0.490 Test: 0.533\n",
      "104 16588.65 Train: 0.421 Val: 0.434 Test: 0.471 -- Best Val: 0.490 Test: 0.533\n",
      "105 16557.629 Train: 0.414 Val: 0.429 Test: 0.462 -- Best Val: 0.490 Test: 0.533\n",
      "106 16595.982 Train: 0.410 Val: 0.427 Test: 0.457 -- Best Val: 0.490 Test: 0.533\n",
      "107 16565.545 Train: 0.414 Val: 0.424 Test: 0.455 -- Best Val: 0.490 Test: 0.533\n",
      "108 16557.998 Train: 0.415 Val: 0.425 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "109 16541.13 Train: 0.415 Val: 0.423 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "110 16550.658 Train: 0.411 Val: 0.424 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "111 16560.775 Train: 0.411 Val: 0.424 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "112 16547.896 Train: 0.407 Val: 0.420 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "113 16518.328 Train: 0.404 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "114 16540.191 Train: 0.409 Val: 0.422 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "115 16527.08 Train: 0.411 Val: 0.425 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "116 16532.846 Train: 0.408 Val: 0.423 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "117 16526.566 Train: 0.410 Val: 0.425 Test: 0.454 -- Best Val: 0.490 Test: 0.533\n",
      "118 16532.562 Train: 0.410 Val: 0.424 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "119 16546.8 Train: 0.410 Val: 0.425 Test: 0.457 -- Best Val: 0.490 Test: 0.533\n",
      "120 16532.254 Train: 0.411 Val: 0.426 Test: 0.455 -- Best Val: 0.490 Test: 0.533\n",
      "121 16514.402 Train: 0.409 Val: 0.425 Test: 0.459 -- Best Val: 0.490 Test: 0.533\n",
      "122 16514.637 Train: 0.408 Val: 0.424 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "123 16464.697 Train: 0.408 Val: 0.424 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "124 16494.074 Train: 0.408 Val: 0.424 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "125 16514.01 Train: 0.409 Val: 0.424 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "126 16484.316 Train: 0.408 Val: 0.423 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "127 16493.959 Train: 0.409 Val: 0.424 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "128 16464.504 Train: 0.408 Val: 0.423 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "129 16493.316 Train: 0.408 Val: 0.422 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "130 16474.404 Train: 0.407 Val: 0.423 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "131 16468.453 Train: 0.407 Val: 0.423 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "132 16449.871 Train: 0.408 Val: 0.423 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "133 16461.287 Train: 0.408 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "134 16472.139 Train: 0.409 Val: 0.423 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "135 16454.953 Train: 0.409 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "136 16452.092 Train: 0.408 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "137 16442.66 Train: 0.412 Val: 0.423 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "138 16451.164 Train: 0.411 Val: 0.425 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "139 16450.498 Train: 0.411 Val: 0.424 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "140 16423.635 Train: 0.411 Val: 0.425 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "141 16466.498 Train: 0.409 Val: 0.423 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "142 16434.24 Train: 0.410 Val: 0.422 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "143 16456.273 Train: 0.411 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "144 16412.994 Train: 0.411 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "145 16431.904 Train: 0.410 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "146 16398.213 Train: 0.405 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "147 16416.326 Train: 0.405 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "148 16417.36 Train: 0.406 Val: 0.418 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "149 16409.094 Train: 0.407 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "150 16382.011 Train: 0.408 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "151 16401.27 Train: 0.408 Val: 0.423 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "152 16376.357 Train: 0.410 Val: 0.423 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "153 16387.865 Train: 0.410 Val: 0.425 Test: 0.455 -- Best Val: 0.490 Test: 0.533\n",
      "154 16394.404 Train: 0.409 Val: 0.422 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "155 16394.395 Train: 0.407 Val: 0.424 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "156 16393.402 Train: 0.408 Val: 0.427 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "157 16372.146 Train: 0.410 Val: 0.424 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "158 16400.42 Train: 0.409 Val: 0.425 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "159 16371.936 Train: 0.408 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "160 16358.102 Train: 0.408 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "161 16375.72 Train: 0.410 Val: 0.424 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "162 16402.156 Train: 0.409 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "163 16393.756 Train: 0.404 Val: 0.413 Test: 0.431 -- Best Val: 0.490 Test: 0.533\n",
      "164 16356.64 Train: 0.406 Val: 0.414 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "165 16366.807 Train: 0.407 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "166 16362.175 Train: 0.408 Val: 0.414 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "167 16371.432 Train: 0.408 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "168 16363.991 Train: 0.405 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "169 16349.907 Train: 0.408 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "170 16341.314 Train: 0.405 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "171 16330.122 Train: 0.407 Val: 0.413 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "172 16338.186 Train: 0.407 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "173 16337.565 Train: 0.409 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "174 16331.645 Train: 0.410 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "175 16321.052 Train: 0.410 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "176 16320.843 Train: 0.410 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "177 16317.83 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "178 16313.008 Train: 0.408 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "179 16312.845 Train: 0.408 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "180 16292.236 Train: 0.410 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "181 16295.168 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "182 16332.591 Train: 0.408 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "183 16345.361 Train: 0.407 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "184 16305.135 Train: 0.409 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "185 16297.885 Train: 0.408 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "186 16324.644 Train: 0.410 Val: 0.423 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "187 16321.629 Train: 0.413 Val: 0.425 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "188 16321.166 Train: 0.411 Val: 0.425 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "189 16295.295 Train: 0.410 Val: 0.423 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "190 16320.099 Train: 0.407 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "191 16315.223 Train: 0.408 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "192 16298.46 Train: 0.410 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "193 16334.628 Train: 0.413 Val: 0.425 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "194 16285.715 Train: 0.412 Val: 0.422 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "195 16303.807 Train: 0.411 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "196 16315.52 Train: 0.412 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "197 16302.159 Train: 0.406 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "198 16291.577 Train: 0.408 Val: 0.423 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "199 16278.905 Train: 0.411 Val: 0.423 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "200 16253.671 Train: 0.412 Val: 0.424 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "201 16275.414 Train: 0.406 Val: 0.420 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "202 16274.978 Train: 0.411 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "203 16247.154 Train: 0.409 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "204 16295.628 Train: 0.406 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "205 16264.213 Train: 0.406 Val: 0.416 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "206 16287.527 Train: 0.406 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "207 16270.584 Train: 0.411 Val: 0.423 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "208 16266.429 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "209 16287.026 Train: 0.406 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "210 16277.213 Train: 0.411 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "211 16272.087 Train: 0.407 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "212 16258.225 Train: 0.410 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "213 16254.337 Train: 0.406 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "214 16244.684 Train: 0.407 Val: 0.421 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "215 16245.239 Train: 0.407 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "216 16250.92 Train: 0.406 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "217 16239.813 Train: 0.409 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "218 16243.503 Train: 0.411 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "219 16259.293 Train: 0.409 Val: 0.423 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "220 16242.395 Train: 0.408 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "221 16253.016 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "222 16254.457 Train: 0.406 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "223 16238.139 Train: 0.411 Val: 0.423 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "224 16247.546 Train: 0.407 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "225 16244.382 Train: 0.411 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "226 16244.967 Train: 0.411 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "227 16214.536 Train: 0.413 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "228 16239.9 Train: 0.411 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "229 16231.886 Train: 0.410 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "230 16242.6875 Train: 0.412 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "231 16230.691 Train: 0.411 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "232 16209.018 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "233 16257.389 Train: 0.418 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "234 16253.823 Train: 0.409 Val: 0.417 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "235 16209.281 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "236 16259.139 Train: 0.410 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "237 16220.105 Train: 0.413 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "238 16212.05 Train: 0.412 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "239 16198.793 Train: 0.410 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "240 16217.995 Train: 0.411 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "241 16211.849 Train: 0.411 Val: 0.418 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "242 16199.0 Train: 0.413 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "243 16215.858 Train: 0.410 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "244 16239.498 Train: 0.411 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "245 16227.078 Train: 0.412 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "246 16201.953 Train: 0.412 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "247 16227.082 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "248 16214.392 Train: 0.410 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "249 16208.194 Train: 0.414 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "250 16218.362 Train: 0.409 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "251 16210.883 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "252 16245.111 Train: 0.410 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "253 16226.885 Train: 0.412 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "254 16219.69 Train: 0.412 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "255 16209.253 Train: 0.412 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "256 16182.603 Train: 0.412 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "257 16200.852 Train: 0.411 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "258 16204.64 Train: 0.405 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "259 16213.346 Train: 0.410 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "260 16183.56 Train: 0.411 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "261 16209.171 Train: 0.412 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "262 16192.284 Train: 0.411 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "263 16187.818 Train: 0.417 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "264 16187.284 Train: 0.416 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "265 16213.261 Train: 0.412 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "266 16198.264 Train: 0.418 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "267 16217.466 Train: 0.412 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "268 16176.093 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "269 16200.675 Train: 0.414 Val: 0.421 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "270 16217.827 Train: 0.414 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "271 16198.097 Train: 0.410 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "272 16202.005 Train: 0.413 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "273 16210.232 Train: 0.410 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "274 16185.656 Train: 0.412 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "275 16194.446 Train: 0.412 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "276 16195.193 Train: 0.412 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "277 16206.046 Train: 0.409 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "278 16179.934 Train: 0.411 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "279 16183.936 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "280 16201.66 Train: 0.413 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "281 16159.836 Train: 0.410 Val: 0.414 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "282 16184.102 Train: 0.414 Val: 0.416 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "283 16188.369 Train: 0.411 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "284 16184.009 Train: 0.411 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "285 16211.186 Train: 0.407 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "286 16191.755 Train: 0.411 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "287 16188.981 Train: 0.408 Val: 0.412 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "288 16172.165 Train: 0.410 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "289 16199.264 Train: 0.408 Val: 0.413 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "290 16176.037 Train: 0.408 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "291 16183.527 Train: 0.410 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "292 16155.592 Train: 0.410 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "293 16160.861 Train: 0.408 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "294 16172.684 Train: 0.409 Val: 0.414 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "295 16187.689 Train: 0.409 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "296 16175.124 Train: 0.409 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "297 16215.368 Train: 0.407 Val: 0.413 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "298 16188.375 Train: 0.409 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "299 16180.013 Train: 0.408 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "300 16166.07 Train: 0.412 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "301 16189.765 Train: 0.408 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "302 16158.345 Train: 0.409 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "303 16178.965 Train: 0.410 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "304 16165.424 Train: 0.409 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "305 16164.567 Train: 0.409 Val: 0.415 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "306 16188.845 Train: 0.411 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "307 16175.863 Train: 0.408 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "308 16194.613 Train: 0.405 Val: 0.413 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "309 16170.235 Train: 0.407 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "310 16167.408 Train: 0.405 Val: 0.413 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "311 16186.466 Train: 0.408 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "312 16168.583 Train: 0.409 Val: 0.419 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "313 16154.401 Train: 0.407 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "314 16156.146 Train: 0.408 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "315 16194.737 Train: 0.407 Val: 0.415 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "316 16166.718 Train: 0.410 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "317 16166.678 Train: 0.410 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "318 16169.132 Train: 0.407 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "319 16161.911 Train: 0.410 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "320 16154.135 Train: 0.408 Val: 0.415 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "321 16150.421 Train: 0.408 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "322 16178.609 Train: 0.409 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "323 16173.972 Train: 0.407 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "324 16152.84 Train: 0.409 Val: 0.416 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "325 16160.279 Train: 0.407 Val: 0.417 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "326 16170.048 Train: 0.412 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "327 16171.197 Train: 0.405 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "328 16167.967 Train: 0.413 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "329 16156.435 Train: 0.413 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "330 16154.678 Train: 0.416 Val: 0.422 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "331 16169.687 Train: 0.411 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "332 16179.441 Train: 0.416 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "333 16168.652 Train: 0.413 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "334 16176.795 Train: 0.415 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "335 16168.753 Train: 0.412 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "336 16150.501 Train: 0.411 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "337 16154.382 Train: 0.412 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "338 16162.021 Train: 0.411 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "339 16132.777 Train: 0.410 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "340 16155.339 Train: 0.411 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "341 16146.922 Train: 0.411 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "342 16170.257 Train: 0.408 Val: 0.414 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "343 16151.528 Train: 0.410 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "344 16163.523 Train: 0.412 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "345 16168.417 Train: 0.413 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "346 16144.453 Train: 0.412 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "347 16133.055 Train: 0.412 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "348 16156.803 Train: 0.416 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "349 16158.612 Train: 0.410 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "350 16150.27 Train: 0.414 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "351 16157.094 Train: 0.414 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "352 16157.644 Train: 0.411 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "353 16162.885 Train: 0.412 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "354 16162.212 Train: 0.415 Val: 0.422 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "355 16145.625 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "356 16159.508 Train: 0.409 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "357 16142.737 Train: 0.409 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "358 16154.388 Train: 0.407 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "359 16160.132 Train: 0.406 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "360 16140.514 Train: 0.409 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "361 16154.5 Train: 0.413 Val: 0.423 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "362 16161.412 Train: 0.413 Val: 0.424 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "363 16173.81 Train: 0.412 Val: 0.422 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "364 16165.168 Train: 0.411 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "365 16144.225 Train: 0.410 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "366 16152.759 Train: 0.410 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "367 16154.273 Train: 0.409 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "368 16131.081 Train: 0.407 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "369 16140.355 Train: 0.408 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "370 16141.17 Train: 0.410 Val: 0.420 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "371 16149.759 Train: 0.415 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "372 16142.407 Train: 0.412 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "373 16165.544 Train: 0.411 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "374 16159.991 Train: 0.415 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "375 16138.121 Train: 0.407 Val: 0.416 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "376 16138.217 Train: 0.410 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "377 16122.691 Train: 0.413 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "378 16156.002 Train: 0.413 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "379 16150.321 Train: 0.411 Val: 0.413 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "380 16132.326 Train: 0.411 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "381 16139.485 Train: 0.411 Val: 0.412 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "382 16141.879 Train: 0.410 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "383 16148.427 Train: 0.413 Val: 0.422 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "384 16147.058 Train: 0.410 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "385 16156.781 Train: 0.414 Val: 0.422 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "386 16145.207 Train: 0.416 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "387 16128.924 Train: 0.415 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "388 16153.834 Train: 0.411 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "389 16137.572 Train: 0.410 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "390 16145.381 Train: 0.413 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "391 16152.783 Train: 0.413 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "392 16137.863 Train: 0.412 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "393 16137.451 Train: 0.417 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "394 16136.744 Train: 0.414 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "395 16146.206 Train: 0.414 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "396 16167.415 Train: 0.407 Val: 0.412 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "397 16154.777 Train: 0.415 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "398 16148.809 Train: 0.407 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "399 16142.711 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "400 16152.812 Train: 0.411 Val: 0.420 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "401 16130.112 Train: 0.408 Val: 0.417 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "402 16150.17 Train: 0.407 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "403 16141.337 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "404 16134.755 Train: 0.410 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "405 16139.863 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "406 16151.492 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "407 16145.9795 Train: 0.413 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "408 16136.389 Train: 0.409 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "409 16138.981 Train: 0.409 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "410 16130.537 Train: 0.412 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "411 16138.3 Train: 0.410 Val: 0.420 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "412 16147.987 Train: 0.411 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "413 16127.823 Train: 0.409 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "414 16139.895 Train: 0.413 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "415 16164.781 Train: 0.412 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "416 16119.257 Train: 0.410 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "417 16166.775 Train: 0.407 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "418 16133.545 Train: 0.412 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "419 16136.744 Train: 0.410 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "420 16132.213 Train: 0.414 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "421 16158.641 Train: 0.413 Val: 0.422 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "422 16141.9375 Train: 0.412 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "423 16126.575 Train: 0.414 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "424 16116.61 Train: 0.412 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "425 16145.57 Train: 0.417 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "426 16136.643 Train: 0.415 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "427 16121.835 Train: 0.413 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "428 16158.938 Train: 0.416 Val: 0.423 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "429 16126.304 Train: 0.416 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "430 16139.141 Train: 0.417 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "431 16128.895 Train: 0.413 Val: 0.421 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "432 16136.492 Train: 0.412 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "433 16144.332 Train: 0.415 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "434 16147.138 Train: 0.410 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "435 16134.724 Train: 0.411 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "436 16141.761 Train: 0.410 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "437 16122.903 Train: 0.410 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "438 16139.103 Train: 0.410 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "439 16125.304 Train: 0.411 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "440 16120.08 Train: 0.411 Val: 0.417 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "441 16128.484 Train: 0.411 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "442 16119.967 Train: 0.412 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "443 16156.576 Train: 0.412 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "444 16157.695 Train: 0.416 Val: 0.417 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "445 16129.28 Train: 0.414 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "446 16134.271 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "447 16134.904 Train: 0.410 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "448 16142.804 Train: 0.415 Val: 0.421 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "449 16150.618 Train: 0.413 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "450 16132.056 Train: 0.413 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "451 16132.312 Train: 0.410 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "452 16115.786 Train: 0.412 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "453 16120.273 Train: 0.412 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "454 16131.421 Train: 0.409 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "455 16145.418 Train: 0.414 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "456 16137.025 Train: 0.405 Val: 0.418 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "457 16127.287 Train: 0.412 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "458 16141.514 Train: 0.411 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "459 16144.127 Train: 0.413 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "460 16139.151 Train: 0.412 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "461 16128.816 Train: 0.412 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "462 16127.174 Train: 0.410 Val: 0.416 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "463 16138.127 Train: 0.413 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "464 16120.726 Train: 0.413 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "465 16127.156 Train: 0.409 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "466 16128.474 Train: 0.409 Val: 0.417 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "467 16131.735 Train: 0.409 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "468 16121.728 Train: 0.413 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "469 16132.667 Train: 0.409 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "470 16123.584 Train: 0.412 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "471 16112.989 Train: 0.411 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "472 16125.801 Train: 0.411 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "473 16129.855 Train: 0.416 Val: 0.423 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "474 16127.565 Train: 0.415 Val: 0.418 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "475 16121.195 Train: 0.416 Val: 0.422 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "476 16133.553 Train: 0.417 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "477 16124.844 Train: 0.416 Val: 0.421 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "478 16139.027 Train: 0.416 Val: 0.419 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "479 16135.218 Train: 0.414 Val: 0.424 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "480 16111.38 Train: 0.414 Val: 0.421 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "481 16130.5625 Train: 0.419 Val: 0.427 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "482 16131.148 Train: 0.417 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "483 16155.045 Train: 0.412 Val: 0.418 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "484 16791.057 Train: 0.392 Val: 0.408 Test: 0.415 -- Best Val: 0.490 Test: 0.533\n",
      "485 16426.691 Train: 0.408 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "486 16275.073 Train: 0.407 Val: 0.417 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "487 16219.3545 Train: 0.407 Val: 0.419 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "488 16119.885 Train: 0.417 Val: 0.424 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "489 16123.795 Train: 0.415 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "490 16126.679 Train: 0.413 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "491 16108.025 Train: 0.413 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "492 16119.405 Train: 0.413 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "493 16123.331 Train: 0.411 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "494 16130.185 Train: 0.412 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "495 16121.8955 Train: 0.414 Val: 0.422 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "496 16127.103 Train: 0.414 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "497 16124.444 Train: 0.410 Val: 0.420 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "498 16113.253 Train: 0.408 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "499 16129.211 Train: 0.409 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "500 16117.439 Train: 0.411 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "501 16126.812 Train: 0.413 Val: 0.421 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "502 16116.502 Train: 0.411 Val: 0.423 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "503 16124.407 Train: 0.409 Val: 0.421 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "504 16115.169 Train: 0.409 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "505 16110.195 Train: 0.408 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "506 16118.264 Train: 0.408 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "507 16118.8125 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "508 16133.801 Train: 0.408 Val: 0.415 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "509 16118.552 Train: 0.406 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "510 16128.972 Train: 0.412 Val: 0.417 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "511 16126.47 Train: 0.409 Val: 0.416 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "512 16114.56 Train: 0.410 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "513 16115.832 Train: 0.412 Val: 0.421 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "514 16113.602 Train: 0.412 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "515 16126.296 Train: 0.409 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "516 16117.504 Train: 0.412 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "517 16115.58 Train: 0.408 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "518 16111.452 Train: 0.414 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "519 16124.015 Train: 0.415 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "520 16128.844 Train: 0.412 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "521 16113.1875 Train: 0.413 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "522 16113.95 Train: 0.411 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "523 16128.998 Train: 0.412 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "524 16107.843 Train: 0.412 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "525 16115.412 Train: 0.410 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "526 16124.331 Train: 0.407 Val: 0.412 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "527 16114.451 Train: 0.409 Val: 0.413 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "528 16116.615 Train: 0.406 Val: 0.413 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "529 16108.44 Train: 0.405 Val: 0.411 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "530 16116.38 Train: 0.407 Val: 0.412 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "531 16120.407 Train: 0.407 Val: 0.411 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "532 16123.597 Train: 0.407 Val: 0.411 Test: 0.431 -- Best Val: 0.490 Test: 0.533\n",
      "533 16112.379 Train: 0.409 Val: 0.413 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "534 16122.31 Train: 0.408 Val: 0.413 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "535 16117.665 Train: 0.408 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "536 16141.189 Train: 0.411 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "537 16134.47 Train: 0.409 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "538 16125.136 Train: 0.412 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "539 16145.299 Train: 0.414 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "540 16117.643 Train: 0.412 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "541 16119.709 Train: 0.409 Val: 0.413 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "542 16114.712 Train: 0.408 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "543 16119.899 Train: 0.410 Val: 0.418 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "544 16099.778 Train: 0.408 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "545 16113.651 Train: 0.409 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "546 16112.834 Train: 0.411 Val: 0.419 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "547 16132.801 Train: 0.411 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "548 16110.796 Train: 0.415 Val: 0.422 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "549 16115.55 Train: 0.412 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "550 16132.406 Train: 0.411 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "551 16111.727 Train: 0.412 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "552 16119.084 Train: 0.415 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "553 16116.982 Train: 0.409 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "554 16106.765 Train: 0.416 Val: 0.417 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "555 16132.074 Train: 0.413 Val: 0.416 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "556 16119.804 Train: 0.413 Val: 0.419 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "557 16110.092 Train: 0.410 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "558 16110.99 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "559 16113.949 Train: 0.409 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "560 16109.681 Train: 0.409 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "561 16143.257 Train: 0.416 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "562 16122.859 Train: 0.410 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "563 16143.584 Train: 0.416 Val: 0.423 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "564 16128.611 Train: 0.411 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "565 16118.67 Train: 0.414 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "566 16132.006 Train: 0.410 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "567 16113.224 Train: 0.410 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "568 16126.982 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "569 16119.56 Train: 0.412 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "570 16102.467 Train: 0.412 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "571 16117.113 Train: 0.413 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "572 16135.082 Train: 0.412 Val: 0.424 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "573 16117.639 Train: 0.414 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "574 16120.282 Train: 0.410 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "575 16123.128 Train: 0.410 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "576 16128.144 Train: 0.407 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "577 16126.651 Train: 0.408 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "578 16128.157 Train: 0.409 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "579 16122.693 Train: 0.413 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "580 16132.921 Train: 0.409 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "581 16119.8 Train: 0.409 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "582 16115.83 Train: 0.406 Val: 0.416 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "583 16117.751 Train: 0.408 Val: 0.417 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "584 16108.227 Train: 0.407 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "585 16114.708 Train: 0.409 Val: 0.411 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "586 16101.371 Train: 0.409 Val: 0.413 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "587 16110.339 Train: 0.414 Val: 0.415 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "588 16116.559 Train: 0.411 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "589 16116.811 Train: 0.415 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "590 16129.763 Train: 0.410 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "591 16122.165 Train: 0.411 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "592 16102.191 Train: 0.410 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "593 16119.076 Train: 0.409 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "594 16130.067 Train: 0.413 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "595 16113.464 Train: 0.412 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "596 16116.963 Train: 0.412 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "597 16124.818 Train: 0.414 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "598 16113.09 Train: 0.415 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "599 16112.873 Train: 0.416 Val: 0.418 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "600 16111.787 Train: 0.413 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "601 16117.356 Train: 0.410 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "602 16119.928 Train: 0.413 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "603 16128.622 Train: 0.410 Val: 0.414 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "604 16109.083 Train: 0.408 Val: 0.415 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "605 16130.549 Train: 0.405 Val: 0.413 Test: 0.430 -- Best Val: 0.490 Test: 0.533\n",
      "606 16121.245 Train: 0.414 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "607 16116.215 Train: 0.413 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "608 16115.836 Train: 0.410 Val: 0.418 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "609 16121.001 Train: 0.411 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "610 16133.04 Train: 0.409 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "611 16125.182 Train: 0.410 Val: 0.417 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "612 16129.469 Train: 0.415 Val: 0.421 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "613 16122.447 Train: 0.412 Val: 0.422 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "614 16106.09 Train: 0.415 Val: 0.421 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "615 16105.988 Train: 0.412 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "616 16134.338 Train: 0.414 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "617 16123.952 Train: 0.411 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "618 16136.594 Train: 0.410 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "619 16113.603 Train: 0.413 Val: 0.422 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "620 16120.45 Train: 0.406 Val: 0.414 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "621 16118.373 Train: 0.410 Val: 0.415 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "622 16124.153 Train: 0.414 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "623 16110.662 Train: 0.412 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "624 16162.895 Train: 0.417 Val: 0.420 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "625 16108.511 Train: 0.413 Val: 0.420 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "626 16123.516 Train: 0.409 Val: 0.416 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "627 16393.176 Train: 0.407 Val: 0.417 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "628 16141.08 Train: 0.411 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "629 16136.773 Train: 0.409 Val: 0.418 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "630 16107.645 Train: 0.411 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "631 16119.969 Train: 0.409 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "632 16124.002 Train: 0.413 Val: 0.416 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "633 16120.264 Train: 0.416 Val: 0.420 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "634 16112.851 Train: 0.412 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "635 16120.184 Train: 0.412 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "636 16120.979 Train: 0.414 Val: 0.416 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "637 16102.218 Train: 0.411 Val: 0.413 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "638 16108.753 Train: 0.410 Val: 0.416 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "639 16120.895 Train: 0.410 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "640 16115.144 Train: 0.410 Val: 0.414 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "641 16126.421 Train: 0.412 Val: 0.415 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "642 16111.305 Train: 0.409 Val: 0.416 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "643 16114.949 Train: 0.414 Val: 0.419 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "644 16117.17 Train: 0.407 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "645 16116.067 Train: 0.407 Val: 0.420 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "646 16111.448 Train: 0.410 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "647 16109.543 Train: 0.411 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "648 16114.862 Train: 0.408 Val: 0.415 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "649 16123.949 Train: 0.407 Val: 0.416 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "650 16114.605 Train: 0.412 Val: 0.419 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "651 16106.176 Train: 0.410 Val: 0.419 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "652 16131.112 Train: 0.411 Val: 0.420 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "653 16124.307 Train: 0.408 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "654 16130.92 Train: 0.409 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "655 16110.591 Train: 0.407 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "656 16110.468 Train: 0.408 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "657 16109.398 Train: 0.411 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "658 16123.734 Train: 0.413 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "659 16128.892 Train: 0.411 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "660 16109.774 Train: 0.407 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "661 16121.001 Train: 0.416 Val: 0.421 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "662 16128.88 Train: 0.415 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "663 16109.735 Train: 0.413 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "664 16120.484 Train: 0.411 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "665 16126.945 Train: 0.411 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "666 16122.577 Train: 0.418 Val: 0.423 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "667 16122.172 Train: 0.413 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "668 16109.38 Train: 0.415 Val: 0.419 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "669 16110.952 Train: 0.411 Val: 0.419 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "670 16110.587 Train: 0.411 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "671 16124.598 Train: 0.410 Val: 0.420 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "672 16115.757 Train: 0.409 Val: 0.420 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "673 16122.87 Train: 0.413 Val: 0.422 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "674 16107.004 Train: 0.407 Val: 0.422 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "675 16115.443 Train: 0.409 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "676 16116.513 Train: 0.407 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "677 16109.916 Train: 0.408 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "678 16108.517 Train: 0.406 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "679 16118.11 Train: 0.408 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "680 16114.357 Train: 0.415 Val: 0.420 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "681 16112.979 Train: 0.414 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "682 16113.609 Train: 0.407 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "683 16108.571 Train: 0.410 Val: 0.420 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "684 16131.806 Train: 0.410 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "685 16121.028 Train: 0.410 Val: 0.422 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "686 16112.769 Train: 0.411 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "687 16113.918 Train: 0.412 Val: 0.421 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "688 16108.528 Train: 0.408 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "689 16105.654 Train: 0.407 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "690 16113.073 Train: 0.409 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "691 16106.303 Train: 0.408 Val: 0.421 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "692 16134.169 Train: 0.411 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "693 16111.361 Train: 0.411 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "694 16120.2 Train: 0.413 Val: 0.419 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "695 16105.607 Train: 0.410 Val: 0.423 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "696 16116.297 Train: 0.408 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "697 16129.307 Train: 0.411 Val: 0.420 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "698 16116.413 Train: 0.412 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "699 16122.415 Train: 0.409 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "700 16114.491 Train: 0.413 Val: 0.419 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "701 16127.753 Train: 0.407 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "702 16108.214 Train: 0.413 Val: 0.421 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "703 16109.127 Train: 0.409 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "704 16120.741 Train: 0.407 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "705 16092.771 Train: 0.414 Val: 0.422 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "706 16109.548 Train: 0.412 Val: 0.418 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "707 16124.435 Train: 0.414 Val: 0.417 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "708 16113.1875 Train: 0.411 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "709 16121.214 Train: 0.416 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "710 16111.417 Train: 0.413 Val: 0.423 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "711 16120.581 Train: 0.414 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "712 16113.79 Train: 0.415 Val: 0.423 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "713 16107.794 Train: 0.414 Val: 0.423 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "714 16118.051 Train: 0.416 Val: 0.422 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "715 16119.147 Train: 0.417 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "716 16112.561 Train: 0.414 Val: 0.421 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "717 16115.818 Train: 0.416 Val: 0.423 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "718 16106.434 Train: 0.414 Val: 0.422 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "719 16125.4 Train: 0.414 Val: 0.425 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "720 16120.021 Train: 0.415 Val: 0.421 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "721 16108.304 Train: 0.411 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "722 16118.4375 Train: 0.412 Val: 0.422 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "723 16111.621 Train: 0.414 Val: 0.423 Test: 0.453 -- Best Val: 0.490 Test: 0.533\n",
      "724 16117.755 Train: 0.416 Val: 0.422 Test: 0.458 -- Best Val: 0.490 Test: 0.533\n",
      "725 16113.18 Train: 0.413 Val: 0.419 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "726 16116.781 Train: 0.415 Val: 0.427 Test: 0.456 -- Best Val: 0.490 Test: 0.533\n",
      "727 16109.494 Train: 0.414 Val: 0.423 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "728 16111.124 Train: 0.413 Val: 0.423 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "729 16123.564 Train: 0.415 Val: 0.426 Test: 0.455 -- Best Val: 0.490 Test: 0.533\n",
      "730 16108.934 Train: 0.415 Val: 0.421 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "731 16113.214 Train: 0.417 Val: 0.420 Test: 0.454 -- Best Val: 0.490 Test: 0.533\n",
      "732 16120.871 Train: 0.414 Val: 0.423 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "733 16120.288 Train: 0.415 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "734 16103.838 Train: 0.415 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "735 16119.198 Train: 0.410 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "736 16107.96 Train: 0.409 Val: 0.417 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "737 16103.46 Train: 0.412 Val: 0.420 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "738 16118.051 Train: 0.413 Val: 0.422 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "739 16104.491 Train: 0.414 Val: 0.420 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "740 16139.13 Train: 0.417 Val: 0.421 Test: 0.454 -- Best Val: 0.490 Test: 0.533\n",
      "741 16864.318 Train: 0.408 Val: 0.413 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "742 16190.269 Train: 0.408 Val: 0.417 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "743 16196.061 Train: 0.410 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "744 16170.764 Train: 0.414 Val: 0.418 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "745 16107.895 Train: 0.415 Val: 0.420 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "746 16111.5625 Train: 0.412 Val: 0.419 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "747 16111.927 Train: 0.412 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "748 16123.141 Train: 0.412 Val: 0.421 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "749 16121.537 Train: 0.411 Val: 0.418 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "750 16113.82 Train: 0.411 Val: 0.416 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "751 16122.621 Train: 0.412 Val: 0.420 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "752 16112.157 Train: 0.408 Val: 0.415 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "753 16110.749 Train: 0.411 Val: 0.416 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "754 16117.739 Train: 0.405 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "755 16101.714 Train: 0.412 Val: 0.417 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "756 16109.031 Train: 0.409 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "757 16114.797 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "758 16116.088 Train: 0.413 Val: 0.417 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "759 16118.104 Train: 0.413 Val: 0.419 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "760 16103.297 Train: 0.413 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "761 16116.486 Train: 0.414 Val: 0.422 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "762 16104.6 Train: 0.415 Val: 0.420 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "763 16114.804 Train: 0.410 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "764 16096.454 Train: 0.412 Val: 0.422 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "765 16115.517 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "766 16112.054 Train: 0.413 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "767 16117.477 Train: 0.413 Val: 0.421 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "768 16118.796 Train: 0.415 Val: 0.423 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "769 16115.831 Train: 0.411 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "770 16102.585 Train: 0.413 Val: 0.418 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "771 16108.186 Train: 0.415 Val: 0.420 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "772 16112.914 Train: 0.411 Val: 0.417 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "773 16107.346 Train: 0.413 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "774 16114.241 Train: 0.412 Val: 0.415 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "775 16105.425 Train: 0.413 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "776 16108.326 Train: 0.411 Val: 0.414 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "777 16114.8955 Train: 0.414 Val: 0.415 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "778 16114.91 Train: 0.412 Val: 0.415 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "779 16107.142 Train: 0.414 Val: 0.416 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "780 16102.944 Train: 0.412 Val: 0.418 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "781 16106.46 Train: 0.413 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "782 16113.032 Train: 0.412 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "783 16101.08 Train: 0.412 Val: 0.420 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "784 16117.591 Train: 0.412 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "785 16124.125 Train: 0.409 Val: 0.414 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "786 16110.1045 Train: 0.411 Val: 0.416 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "787 16118.749 Train: 0.409 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "788 16124.362 Train: 0.410 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "789 16113.578 Train: 0.411 Val: 0.417 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "790 16117.534 Train: 0.411 Val: 0.418 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "791 16117.373 Train: 0.407 Val: 0.416 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "792 16105.99 Train: 0.407 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "793 16120.774 Train: 0.412 Val: 0.416 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "794 16110.491 Train: 0.411 Val: 0.419 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "795 16112.173 Train: 0.411 Val: 0.418 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "796 16102.578 Train: 0.410 Val: 0.417 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "797 16101.434 Train: 0.407 Val: 0.416 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "798 16106.902 Train: 0.407 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "799 16115.73 Train: 0.410 Val: 0.417 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "800 16111.137 Train: 0.410 Val: 0.418 Test: 0.452 -- Best Val: 0.490 Test: 0.533\n",
      "801 16104.7705 Train: 0.411 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "802 16095.344 Train: 0.412 Val: 0.416 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "803 16115.958 Train: 0.410 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "804 16107.649 Train: 0.411 Val: 0.415 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "805 16106.862 Train: 0.408 Val: 0.419 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "806 16110.496 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "807 16120.282 Train: 0.411 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "808 16120.7705 Train: 0.409 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "809 16106.116 Train: 0.411 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "810 16111.998 Train: 0.408 Val: 0.413 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "811 16115.424 Train: 0.408 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "812 16110.387 Train: 0.409 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "813 16109.481 Train: 0.409 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "814 16105.027 Train: 0.409 Val: 0.421 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "815 16115.232 Train: 0.411 Val: 0.415 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "816 16118.979 Train: 0.410 Val: 0.416 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "817 16097.959 Train: 0.408 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "818 16105.374 Train: 0.404 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "819 16112.911 Train: 0.406 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "820 16109.988 Train: 0.407 Val: 0.413 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "821 16111.64 Train: 0.409 Val: 0.415 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "822 16121.251 Train: 0.407 Val: 0.416 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "823 16104.114 Train: 0.404 Val: 0.415 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "824 16114.97 Train: 0.406 Val: 0.414 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "825 16101.237 Train: 0.408 Val: 0.419 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "826 16100.74 Train: 0.405 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "827 16105.205 Train: 0.406 Val: 0.415 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "828 16097.552 Train: 0.405 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "829 16281.971 Train: 0.410 Val: 0.419 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "830 16174.511 Train: 0.410 Val: 0.419 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "831 16151.406 Train: 0.408 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "832 16108.335 Train: 0.408 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "833 16112.472 Train: 0.406 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "834 16124.785 Train: 0.410 Val: 0.416 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "835 16124.379 Train: 0.407 Val: 0.412 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "836 16113.152 Train: 0.407 Val: 0.413 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "837 16140.629 Train: 0.413 Val: 0.422 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "838 16113.289 Train: 0.408 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "839 16141.679 Train: 0.408 Val: 0.415 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "840 16109.876 Train: 0.411 Val: 0.418 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "841 16144.701 Train: 0.409 Val: 0.418 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "842 16132.343 Train: 0.407 Val: 0.413 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "843 16124.306 Train: 0.409 Val: 0.417 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "844 16115.537 Train: 0.412 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "845 16111.106 Train: 0.412 Val: 0.418 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "846 16118.841 Train: 0.410 Val: 0.419 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "847 16117.164 Train: 0.412 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "848 16117.191 Train: 0.410 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "849 16107.497 Train: 0.410 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "850 16109.312 Train: 0.409 Val: 0.417 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "851 16102.18 Train: 0.411 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "852 16106.253 Train: 0.408 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "853 16105.753 Train: 0.408 Val: 0.415 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "854 16109.172 Train: 0.409 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "855 16125.6 Train: 0.408 Val: 0.416 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "856 16114.743 Train: 0.410 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "857 16117.073 Train: 0.412 Val: 0.416 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "858 16108.045 Train: 0.409 Val: 0.413 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "859 16123.367 Train: 0.410 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "860 16103.41 Train: 0.411 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "861 16111.151 Train: 0.409 Val: 0.413 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "862 16109.914 Train: 0.411 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "863 16107.377 Train: 0.410 Val: 0.414 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "864 16100.984 Train: 0.409 Val: 0.414 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "865 16100.975 Train: 0.411 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "866 16118.252 Train: 0.411 Val: 0.415 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "867 16121.889 Train: 0.410 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "868 16116.42 Train: 0.413 Val: 0.416 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "869 16103.21 Train: 0.407 Val: 0.411 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "870 16106.379 Train: 0.408 Val: 0.413 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "871 16101.874 Train: 0.407 Val: 0.411 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "872 16125.922 Train: 0.406 Val: 0.410 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "873 16107.356 Train: 0.408 Val: 0.409 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "874 16101.89 Train: 0.405 Val: 0.408 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "875 16109.11 Train: 0.403 Val: 0.407 Test: 0.431 -- Best Val: 0.490 Test: 0.533\n",
      "876 16115.108 Train: 0.404 Val: 0.407 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "877 16116.363 Train: 0.403 Val: 0.412 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "878 16121.308 Train: 0.403 Val: 0.411 Test: 0.430 -- Best Val: 0.490 Test: 0.533\n",
      "879 16107.264 Train: 0.403 Val: 0.407 Test: 0.432 -- Best Val: 0.490 Test: 0.533\n",
      "880 16120.52 Train: 0.407 Val: 0.412 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "881 16113.77 Train: 0.408 Val: 0.410 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "882 16100.677 Train: 0.404 Val: 0.409 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "883 16117.316 Train: 0.403 Val: 0.408 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "884 16105.8 Train: 0.406 Val: 0.411 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "885 16111.949 Train: 0.407 Val: 0.409 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "886 16117.639 Train: 0.405 Val: 0.408 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "887 16112.867 Train: 0.410 Val: 0.411 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "888 16113.936 Train: 0.408 Val: 0.410 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "889 16111.92 Train: 0.410 Val: 0.414 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "890 16114.639 Train: 0.407 Val: 0.412 Test: 0.433 -- Best Val: 0.490 Test: 0.533\n",
      "891 16111.154 Train: 0.407 Val: 0.412 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "892 16103.5205 Train: 0.408 Val: 0.412 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "893 16105.95 Train: 0.409 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "894 16113.876 Train: 0.413 Val: 0.416 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "895 16107.727 Train: 0.409 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "896 16114.823 Train: 0.404 Val: 0.410 Test: 0.434 -- Best Val: 0.490 Test: 0.533\n",
      "897 16102.931 Train: 0.409 Val: 0.414 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "898 16116.092 Train: 0.408 Val: 0.414 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "899 16108.841 Train: 0.411 Val: 0.417 Test: 0.448 -- Best Val: 0.490 Test: 0.533\n",
      "900 16112.294 Train: 0.410 Val: 0.413 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "901 16110.905 Train: 0.413 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "902 16109.372 Train: 0.409 Val: 0.413 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "903 16107.123 Train: 0.411 Val: 0.414 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "904 16100.7295 Train: 0.409 Val: 0.412 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "905 16106.691 Train: 0.409 Val: 0.415 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "906 16112.999 Train: 0.408 Val: 0.413 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "907 16105.448 Train: 0.415 Val: 0.418 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "908 16111.639 Train: 0.411 Val: 0.414 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "909 16108.351 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "910 16104.842 Train: 0.407 Val: 0.415 Test: 0.435 -- Best Val: 0.490 Test: 0.533\n",
      "911 16109.566 Train: 0.407 Val: 0.412 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "912 16102.582 Train: 0.406 Val: 0.416 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "913 16116.877 Train: 0.406 Val: 0.413 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "914 16112.642 Train: 0.408 Val: 0.412 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "915 16112.008 Train: 0.411 Val: 0.415 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "916 16108.668 Train: 0.408 Val: 0.414 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "917 16120.457 Train: 0.405 Val: 0.415 Test: 0.431 -- Best Val: 0.490 Test: 0.533\n",
      "918 16106.055 Train: 0.414 Val: 0.413 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "919 16112.53 Train: 0.410 Val: 0.415 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "920 16102.8 Train: 0.413 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "921 16101.1045 Train: 0.413 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "922 16101.165 Train: 0.414 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "923 16108.636 Train: 0.413 Val: 0.416 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "924 16109.6 Train: 0.413 Val: 0.416 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "925 16105.591 Train: 0.415 Val: 0.417 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "926 16124.496 Train: 0.412 Val: 0.415 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "927 16103.401 Train: 0.410 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "928 16101.024 Train: 0.414 Val: 0.417 Test: 0.438 -- Best Val: 0.490 Test: 0.533\n",
      "929 16105.566 Train: 0.414 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "930 16128.339 Train: 0.410 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "931 16110.972 Train: 0.414 Val: 0.417 Test: 0.450 -- Best Val: 0.490 Test: 0.533\n",
      "932 16102.9375 Train: 0.411 Val: 0.414 Test: 0.436 -- Best Val: 0.490 Test: 0.533\n",
      "933 16115.592 Train: 0.409 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "934 16116.818 Train: 0.411 Val: 0.415 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "935 16103.761 Train: 0.412 Val: 0.415 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "936 16101.981 Train: 0.412 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "937 16110.773 Train: 0.413 Val: 0.419 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "938 16112.398 Train: 0.410 Val: 0.417 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "939 16123.848 Train: 0.409 Val: 0.419 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "940 16110.376 Train: 0.415 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "941 16110.914 Train: 0.415 Val: 0.418 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "942 16121.82 Train: 0.413 Val: 0.419 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "943 16103.616 Train: 0.413 Val: 0.422 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "944 16106.063 Train: 0.413 Val: 0.420 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "945 16108.105 Train: 0.413 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "946 16103.125 Train: 0.412 Val: 0.418 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "947 16100.292 Train: 0.410 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "948 16107.02 Train: 0.412 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "949 16761.334 Train: 0.414 Val: 0.423 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "950 16579.912 Train: 0.414 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "951 16228.769 Train: 0.410 Val: 0.417 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "952 16157.847 Train: 0.409 Val: 0.418 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "953 16141.558 Train: 0.409 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "954 16127.533 Train: 0.409 Val: 0.419 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "955 16131.56 Train: 0.411 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "956 16121.723 Train: 0.413 Val: 0.418 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "957 16106.652 Train: 0.412 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "958 16122.429 Train: 0.412 Val: 0.420 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "959 16115.073 Train: 0.411 Val: 0.419 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "960 16124.855 Train: 0.410 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "961 16109.994 Train: 0.415 Val: 0.421 Test: 0.451 -- Best Val: 0.490 Test: 0.533\n",
      "962 16095.794 Train: 0.411 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "963 16114.169 Train: 0.411 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "964 16110.846 Train: 0.409 Val: 0.416 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "965 16113.947 Train: 0.415 Val: 0.421 Test: 0.449 -- Best Val: 0.490 Test: 0.533\n",
      "966 16113.124 Train: 0.412 Val: 0.418 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "967 16115.412 Train: 0.410 Val: 0.419 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "968 16111.979 Train: 0.413 Val: 0.421 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "969 16113.332 Train: 0.408 Val: 0.416 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "970 16122.363 Train: 0.411 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "971 16101.198 Train: 0.412 Val: 0.419 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "972 16107.834 Train: 0.411 Val: 0.418 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "973 16108.074 Train: 0.408 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "974 16107.429 Train: 0.410 Val: 0.418 Test: 0.447 -- Best Val: 0.490 Test: 0.533\n",
      "975 16115.48 Train: 0.408 Val: 0.417 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "976 16107.356 Train: 0.411 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "977 16106.188 Train: 0.412 Val: 0.421 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "978 16115.762 Train: 0.410 Val: 0.418 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "979 16097.965 Train: 0.410 Val: 0.421 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "980 16108.815 Train: 0.409 Val: 0.421 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "981 16106.448 Train: 0.409 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "982 16112.965 Train: 0.408 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "983 16104.95 Train: 0.406 Val: 0.412 Test: 0.439 -- Best Val: 0.490 Test: 0.533\n",
      "984 16107.88 Train: 0.409 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "985 16116.266 Train: 0.409 Val: 0.414 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "986 16105.713 Train: 0.409 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "987 16111.576 Train: 0.411 Val: 0.416 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "988 16105.001 Train: 0.409 Val: 0.416 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "989 16098.232 Train: 0.410 Val: 0.416 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "990 16109.041 Train: 0.411 Val: 0.419 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "991 16114.78 Train: 0.410 Val: 0.415 Test: 0.440 -- Best Val: 0.490 Test: 0.533\n",
      "992 16101.568 Train: 0.410 Val: 0.418 Test: 0.442 -- Best Val: 0.490 Test: 0.533\n",
      "993 16109.887 Train: 0.412 Val: 0.419 Test: 0.446 -- Best Val: 0.490 Test: 0.533\n",
      "994 16110.553 Train: 0.412 Val: 0.420 Test: 0.444 -- Best Val: 0.490 Test: 0.533\n",
      "995 16097.803 Train: 0.410 Val: 0.415 Test: 0.437 -- Best Val: 0.490 Test: 0.533\n",
      "996 16109.769 Train: 0.411 Val: 0.420 Test: 0.445 -- Best Val: 0.490 Test: 0.533\n",
      "997 16099.245 Train: 0.409 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n",
      "998 16118.948 Train: 0.412 Val: 0.417 Test: 0.443 -- Best Val: 0.490 Test: 0.533\n",
      "999 16101.328 Train: 0.411 Val: 0.417 Test: 0.441 -- Best Val: 0.490 Test: 0.533\n"
     ]
    }
   ],
   "source": [
    "train_size = xdata.shape[0]\n",
    "batch_size = 10\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(np.hstack((xdata, ydata)))\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "# print(xdata.shape)\n",
    "num_epoch = 1000\n",
    "i = 0;\n",
    "best_val = 0\n",
    "test_val = 0\n",
    "\n",
    "while i < num_epoch:\n",
    "  loss = 0;\n",
    "  for batch in train_dataset:\n",
    "    xbatch = batch[:, :max_length_output]\n",
    "    ybatch = batch[:, max_length_output:]\n",
    "    loss += train_step(xbatch, ybatch)\n",
    "\n",
    "  ypred_test = cite(xtest, predict=True).numpy()\n",
    "  ypred_val = cite(xval, predict=True).numpy()\n",
    "  ypred_train = cite(xdata, predict=True).numpy()\n",
    "  print(i, loss.numpy(), perf(ypred_train, ypred_val, ypred_test, ydata, yval, ytest) )\n",
    "  i = i+1\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4zAoW660LIM",
    "outputId": "e9372370-e8a3-4c5d-a491-d5903ec5119c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author author \t\t m\n",
      "author author \t\t .\n",
      "author author \t\t kitsuregawa\n",
      "author author \t\t ,\n",
      "author author \t\t h\n",
      "author author \t\t .\n",
      "author author \t\t tanaka\n",
      "author author \t\t ,\n",
      "author author \t\t and\n",
      "author author \t\t t\n",
      "author title \t\t .\n",
      "author title \t\t moto\n",
      "author title \t\t -\n",
      "author title \t\t oka\n",
      "author title \t\t .\n",
      "title title \t\t application\n",
      "title title \t\t of\n",
      "title title \t\t hash\n",
      "title title \t\t to\n",
      "title title \t\t data\n",
      "title title \t\t base\n",
      "title title \t\t machine\n",
      "title title \t\t and\n",
      "title title \t\t its\n",
      "title title \t\t architecture\n",
      "title booktitle \t\t .\n",
      "journal booktitle \t\t new\n",
      "journal booktitle \t\t generation\n",
      "journal booktitle \t\t computing\n",
      "journal booktitle \t\t ,\n",
      "volume booktitle \t\t 1\n",
      "volume pages \t\t (\n",
      "volume booktitle \t\t 1\n",
      "volume pages \t\t )\n",
      "volume PAD \t\t ,\n",
      "date booktitle \t\t 1983\n",
      "date PAD \t\t .\n"
     ]
    }
   ],
   "source": [
    "print_sentence_pred_label(xdata[1], ypred_train[1], ydata[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDwot8oPMYpT",
    "outputId": "9132757a-c20e-436b-f510-feaab2b0c5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editor author \t\t in\n",
      "editor author \t\t bouma\n",
      "editor author \t\t ,\n",
      "editor author \t\t h\n",
      "editor author \t\t .\n",
      "editor author \t\t ,\n",
      "editor author \t\t &\n",
      "editor author \t\t elsendoorn\n",
      "editor author \t\t ,\n",
      "editor author \t\t a\n",
      "editor author \t\t .\n",
      "editor author \t\t g\n",
      "editor title \t\t .\n",
      "editor title \t\t (\n",
      "editor title \t\t eds\n",
      "editor title \t\t .\n",
      "editor title \t\t )\n",
      "editor title \t\t ,\n",
      "title title \t\t working\n",
      "title title \t\t models\n",
      "title title \t\t of\n",
      "title title \t\t human\n",
      "title title \t\t perception\n",
      "title title \t\t ,\n",
      "pages booktitle \t\t pp\n",
      "pages booktitle \t\t .\n",
      "pages booktitle \t\t 391\n",
      "pages booktitle \t\t -\n",
      "pages booktitle \t\t 410\n",
      "pages booktitle \t\t .\n",
      "publisher booktitle \t\t academic\n",
      "publisher pages \t\t press\n",
      "publisher pages \t\t ,\n",
      "location volume \t\t london\n",
      "location PAD \t\t ,\n",
      "location PAD \t\t england\n",
      "location PAD \t\t .\n"
     ]
    }
   ],
   "source": [
    "print_sentence_pred_label(xtest[1], ypred_test[1], ytest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U92_vrUCsKRc"
   },
   "source": [
    "**Model 2**:  Stacked Lstm\n",
    "\n",
    "Accuracy : 43.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "4utQTpUiq9lB"
   },
   "outputs": [],
   "source": [
    "output_size = len(label_dic)\n",
    "max_length_output = 118\n",
    "class CitationNetwork(tf.keras.Model):\n",
    "  def __init__(self, network_type = 'SimpleLSTM'):\n",
    "      super(CitationNetwork, self).__init__()\n",
    "\n",
    "      # self.optimizer = tf.keras.optimizers.RMSprop(1e-3)\n",
    "      self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "      self.embedding = layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_size)\n",
    "      \n",
    "      # self.forward_layer1 = layers.LSTM(32, return_sequences=True, return_state=True)\n",
    "      # self.backward_layer1 = layers.LSTM(32, return_sequences=True,return_state=True,go_backwards=True)\n",
    "      # self.rnn1 = layers.Bidirectional(self.forward_layer1,merge_mode=\"concat\",backward_layer = self.backward_layer1)\n",
    "\n",
    "      # self.forward_layer = layers.LSTM(32, return_sequences=True, return_state=True)\n",
    "      # self.backward_layer = layers.LSTM(32, return_sequences=True,return_state=True,go_backwards=True)\n",
    "      # self.rnn = layers.Bidirectional(self.forward_layer,merge_mode=\"concat\",backward_layer = self.backward_layer)\n",
    "\n",
    "      self.rnn = layers.LSTM(256,return_sequences=True, return_state=True)\n",
    "      self.rnn1 = layers.LSTM(256,return_sequences=True, return_state=True)\n",
    "      self.rnn2 = layers.LSTM(256,return_sequences=True, return_state=True)\n",
    "     \n",
    "\n",
    "      self.fc = layers.Dense(output_size)\n",
    "      \n",
    " \n",
    "  def call(self, x, predict=True):\n",
    "      emb = self.embedding(x)\n",
    "      output1,state_h1,state_c1 = self.rnn(emb)\n",
    "      output2,state_h2,state_c2 = self.rnn1(emb, initial_state = [state_h1,state_c1])\n",
    "      output,state_h3,state_c3 = self.rnn2(emb,initial_state = [state_h2,state_c2])\n",
    "\n",
    "      output = self.fc(output)\n",
    "      if predict:\n",
    "        return tf.math.argmax(output, axis=-1)\n",
    "      return output\n",
    "  \n",
    "  def get_loss(self, ylogits, yt):\n",
    "    # y - one hot vector which makes it easy to compute loss - KC\n",
    "    ylabels = tf.one_hot(ybatch, output_size)\n",
    "    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=ylogits, labels=ylabels);\n",
    "    return tf.reduce_sum(cross_ent)\n",
    "\n",
    "@tf.function\n",
    "def train_step(xbatch, ybatch):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = cite(xbatch, False)\n",
    "        # prediction = cite(xbatch, False)\n",
    "        loss = cite.get_loss(prediction, ybatch)\n",
    "        \n",
    "    gradients = tape.gradient(loss, cite.trainable_variables)\n",
    "    cite.optimizer.apply_gradients(zip(gradients, cite.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "cite = CitationNetwork();\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b5XFoN5XrxvU",
    "outputId": "7d6b1070-da30-4625-fab2-0548e749c214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41848.746 Train: 0.397 Val: 0.397 Test: 0.434 -- Best Val: 0.397 Test: 0.434\n",
      "1 22891.1 Train: 0.378 Val: 0.381 Test: 0.410 -- Best Val: 0.397 Test: 0.434\n",
      "2 20399.193 Train: 0.376 Val: 0.376 Test: 0.401 -- Best Val: 0.397 Test: 0.434\n",
      "3 19073.816 Train: 0.378 Val: 0.382 Test: 0.412 -- Best Val: 0.397 Test: 0.434\n",
      "4 18485.547 Train: 0.384 Val: 0.390 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "5 18211.223 Train: 0.385 Val: 0.391 Test: 0.424 -- Best Val: 0.397 Test: 0.434\n",
      "6 18045.51 Train: 0.383 Val: 0.389 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "7 17957.277 Train: 0.383 Val: 0.389 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "8 17885.637 Train: 0.384 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "9 17822.941 Train: 0.383 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "10 17758.809 Train: 0.384 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "11 17734.717 Train: 0.384 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "12 17680.438 Train: 0.384 Val: 0.389 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "13 17661.848 Train: 0.385 Val: 0.392 Test: 0.423 -- Best Val: 0.397 Test: 0.434\n",
      "14 17676.363 Train: 0.383 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "15 17652.309 Train: 0.380 Val: 0.386 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "16 17664.58 Train: 0.384 Val: 0.390 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "17 17581.303 Train: 0.383 Val: 0.388 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "18 17574.473 Train: 0.381 Val: 0.388 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "19 17563.984 Train: 0.384 Val: 0.389 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "20 17561.959 Train: 0.383 Val: 0.389 Test: 0.423 -- Best Val: 0.397 Test: 0.434\n",
      "21 17540.98 Train: 0.384 Val: 0.389 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "22 17503.855 Train: 0.381 Val: 0.388 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "23 17492.295 Train: 0.379 Val: 0.386 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "24 17489.793 Train: 0.382 Val: 0.389 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "25 17480.53 Train: 0.380 Val: 0.388 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "26 17448.69 Train: 0.382 Val: 0.387 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "27 17500.31 Train: 0.381 Val: 0.385 Test: 0.414 -- Best Val: 0.397 Test: 0.434\n",
      "28 17464.705 Train: 0.383 Val: 0.387 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "29 17419.496 Train: 0.385 Val: 0.387 Test: 0.426 -- Best Val: 0.397 Test: 0.434\n",
      "30 17432.518 Train: 0.380 Val: 0.381 Test: 0.414 -- Best Val: 0.397 Test: 0.434\n",
      "31 17491.781 Train: 0.377 Val: 0.380 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "32 17406.28 Train: 0.382 Val: 0.385 Test: 0.423 -- Best Val: 0.397 Test: 0.434\n",
      "33 17379.064 Train: 0.381 Val: 0.382 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "34 17424.557 Train: 0.378 Val: 0.382 Test: 0.415 -- Best Val: 0.397 Test: 0.434\n",
      "35 17494.373 Train: 0.382 Val: 0.383 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "36 17388.068 Train: 0.378 Val: 0.376 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "37 17362.895 Train: 0.378 Val: 0.378 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "38 17392.375 Train: 0.382 Val: 0.380 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "39 17368.986 Train: 0.380 Val: 0.379 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "40 17342.748 Train: 0.380 Val: 0.381 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "41 17331.896 Train: 0.377 Val: 0.381 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "42 17355.16 Train: 0.376 Val: 0.375 Test: 0.414 -- Best Val: 0.397 Test: 0.434\n",
      "43 17316.85 Train: 0.380 Val: 0.380 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "44 17321.775 Train: 0.380 Val: 0.380 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "45 17320.264 Train: 0.377 Val: 0.378 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "46 17306.502 Train: 0.378 Val: 0.379 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "47 17333.018 Train: 0.373 Val: 0.375 Test: 0.413 -- Best Val: 0.397 Test: 0.434\n",
      "48 17327.977 Train: 0.376 Val: 0.377 Test: 0.415 -- Best Val: 0.397 Test: 0.434\n",
      "49 17304.305 Train: 0.377 Val: 0.379 Test: 0.415 -- Best Val: 0.397 Test: 0.434\n",
      "50 17312.836 Train: 0.381 Val: 0.381 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "51 17318.545 Train: 0.375 Val: 0.379 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "52 17901.281 Train: 0.386 Val: 0.389 Test: 0.426 -- Best Val: 0.397 Test: 0.434\n",
      "53 17382.277 Train: 0.384 Val: 0.385 Test: 0.426 -- Best Val: 0.397 Test: 0.434\n",
      "54 17304.805 Train: 0.382 Val: 0.385 Test: 0.423 -- Best Val: 0.397 Test: 0.434\n",
      "55 17320.002 Train: 0.378 Val: 0.383 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "56 17299.357 Train: 0.377 Val: 0.378 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "57 17290.459 Train: 0.378 Val: 0.377 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "58 17279.975 Train: 0.379 Val: 0.378 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "59 17291.762 Train: 0.382 Val: 0.386 Test: 0.425 -- Best Val: 0.397 Test: 0.434\n",
      "60 17292.34 Train: 0.379 Val: 0.378 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "61 17270.523 Train: 0.378 Val: 0.384 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "62 17279.314 Train: 0.376 Val: 0.376 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "63 17269.734 Train: 0.378 Val: 0.379 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n",
      "64 17254.48 Train: 0.381 Val: 0.381 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "65 17259.75 Train: 0.380 Val: 0.378 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "66 17246.352 Train: 0.380 Val: 0.384 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "67 17276.453 Train: 0.377 Val: 0.377 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "68 17281.66 Train: 0.382 Val: 0.380 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "69 17241.824 Train: 0.378 Val: 0.381 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "70 17239.336 Train: 0.375 Val: 0.379 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "71 17231.516 Train: 0.380 Val: 0.380 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "72 17244.145 Train: 0.376 Val: 0.376 Test: 0.414 -- Best Val: 0.397 Test: 0.434\n",
      "73 17260.475 Train: 0.384 Val: 0.383 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "74 17266.166 Train: 0.380 Val: 0.381 Test: 0.423 -- Best Val: 0.397 Test: 0.434\n",
      "75 17219.955 Train: 0.375 Val: 0.377 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "76 17239.12 Train: 0.378 Val: 0.379 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "77 17221.229 Train: 0.380 Val: 0.379 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "78 17218.463 Train: 0.378 Val: 0.377 Test: 0.415 -- Best Val: 0.397 Test: 0.434\n",
      "79 17243.5 Train: 0.378 Val: 0.379 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "80 17227.701 Train: 0.378 Val: 0.380 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "81 17236.664 Train: 0.380 Val: 0.378 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "82 17234.215 Train: 0.378 Val: 0.383 Test: 0.422 -- Best Val: 0.397 Test: 0.434\n",
      "83 17223.41 Train: 0.382 Val: 0.381 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "84 17228.832 Train: 0.377 Val: 0.383 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "85 17246.004 Train: 0.376 Val: 0.381 Test: 0.421 -- Best Val: 0.397 Test: 0.434\n",
      "86 17238.926 Train: 0.385 Val: 0.389 Test: 0.429 -- Best Val: 0.397 Test: 0.434\n",
      "87 17230.29 Train: 0.374 Val: 0.379 Test: 0.414 -- Best Val: 0.397 Test: 0.434\n",
      "88 17232.479 Train: 0.376 Val: 0.381 Test: 0.418 -- Best Val: 0.397 Test: 0.434\n",
      "89 17208.273 Train: 0.382 Val: 0.381 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "90 17252.637 Train: 0.373 Val: 0.378 Test: 0.415 -- Best Val: 0.397 Test: 0.434\n",
      "91 17229.846 Train: 0.374 Val: 0.380 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "92 19316.148 Train: 0.365 Val: 0.361 Test: 0.398 -- Best Val: 0.397 Test: 0.434\n",
      "93 17679.287 Train: 0.372 Val: 0.373 Test: 0.412 -- Best Val: 0.397 Test: 0.434\n",
      "94 17379.879 Train: 0.376 Val: 0.376 Test: 0.413 -- Best Val: 0.397 Test: 0.434\n",
      "95 17318.742 Train: 0.378 Val: 0.380 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "96 17317.354 Train: 0.378 Val: 0.380 Test: 0.416 -- Best Val: 0.397 Test: 0.434\n",
      "97 17309.64 Train: 0.379 Val: 0.382 Test: 0.420 -- Best Val: 0.397 Test: 0.434\n",
      "98 17288.535 Train: 0.379 Val: 0.381 Test: 0.419 -- Best Val: 0.397 Test: 0.434\n",
      "99 17275.04 Train: 0.378 Val: 0.379 Test: 0.417 -- Best Val: 0.397 Test: 0.434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c6d019d1b0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mxbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmax_length_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mybatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mypred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = xdata.shape[0]\n",
    "batch_size = 10\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(np.hstack((xdata, ydata)))\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "# print(xdata.shape)\n",
    "num_epoch = 1000\n",
    "i = 0;\n",
    "best_val = 0\n",
    "test_val = 0\n",
    "\n",
    "while i < num_epoch:\n",
    "  loss = 0;\n",
    "  for batch in train_dataset:\n",
    "    xbatch = batch[:, :max_length_output]\n",
    "    ybatch = batch[:, max_length_output:]\n",
    "    loss += train_step(xbatch, ybatch)\n",
    "\n",
    "  ypred_test = cite(xtest, predict=True).numpy()\n",
    "  ypred_val = cite(xval, predict=True).numpy()\n",
    "  ypred_train = cite(xdata, predict=True).numpy()\n",
    "  print(i, loss.numpy(), perf(ypred_train, ypred_val, ypred_test, ydata, yval, ytest) )\n",
    "  i = i+1\n",
    "  # break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "citation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
